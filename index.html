<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Language-detect by cloudmark</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Language-detect</h1>
        <p>Language Detection using an N-Gram rank order algorithm.  </p>

        <p class="view"><a href="https://github.com/cloudmark/language-detect">View the Project on GitHub <small>cloudmark/language-detect</small></a></p>


        <ul>
          <li><a href="https://github.com/cloudmark/language-detect/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/cloudmark/language-detect/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/cloudmark/language-detect">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h1>Language Detection</h1>

<blockquote>
<p>Text Adapted from <a href="http://www.lrec-conf.org/proceedings/lrec2010/pdf/279_Paper.pdf">here</a></p>
</blockquote>

<p>This project will outline an N-Gram based language detection using the rank order method described <a href="http://www.lrec-conf.org/proceedings/lrec2010/pdf/279_Paper.pdf">here</a>, It is meant to give the necessary context to understand what is happening and well as a valid implementation. The system is based on calculating and comparing language profiles of N-gram frequencies. First we use the system to compute N-gram profiles on training data files written in target languages - language profiles. Given a novel document we want to classify, we compute the N-gram profile of this document document profile and compute the distance between the document and language profiles.</p>

<h1>So what exactly is an N-Gram?</h1>

<p>N-Grams is an N-character slice of a longer string. Typically one would slice the string into a set of overlapping N-grams. In our system we will use N-Grams of various lengths simultaneously. We will also append blanks to the beginning and end of strings in order to help with matching the beginning-of-word and end-of-word situations. We will represent this using the _ character. Given the word "TEXT" we would obtain the following N-Grams</p>

<ul>
<li>bi-grams <em>T, TE, EX, XT, T</em>
</li>
<li>tri-grams <em>TE, TEX, EXT, XT</em>, T__ </li>
<li>quad-grams <em>TEX, TEXT, EXT</em>, XT<strong>, T</strong>_</li>
</ul><p>In general a string of length k, padded with blanks, will have k + 1 bi-grams, k + 1 tri-grams, k + 1 quad-grams and so on.</p>

<h1>How does this all work?</h1>

<p>Human language has some words which occur more than others. For example you can imagine that in a document the word the will occur more frequently than the word aardvark. More over there is always a small set of words which dominates most of the language in terms of frequency of use. This is true both for words in a particular language and also for words in a particular category. Thus words which appear in a sporty document will be different from the words that appear in a political document and words which are in the English language will obviously be different from words which are in the French language.
Well it turns out that these small fragments of words (N-grams) also obey (approximately) the same property. Thus given a document in a particular language we will always find a set of N-grams which dominate and these set of N-grams will be different for each language. Since we are using small fragments of text we are not very susceptible to noise making our detection more resilient.</p>

<h1>Pre-processing</h1>

<p>Generation of the language profiles is easy. Given an input document the following steps need to be performed.</p>

<ul>
<li><p>Split the input document into separate tokens consisting only of letters and apostrophes. Digits and punctuation should be discarded.</p></li>
<li><p>Remove any extra spacing and make sure that there is always a letter before a punctuation mark. Thus a. is good while a . is bad</p></li>
<li><p>Spit the document into three parts; train, validation and test in the ratio 0.7, 0.2, and 0.1 respectively. Put the partitioned document into three separate folders; train, validate and test. </p></li>
<li><p>Scan down each token generating all possible N-grams, for N = 1 to 5. Use positions that span the padding blanks as well.  </p></li>
<li><p>Use a hash table to find the counter for the particular N-gram and incrementing it. When done sort the counts in reverse order by the number of occurrences and take the top n N-Grams (limit). The result is the N-gram frequency profile of the language.</p></li>
</ul><h1>What to expect</h1>

<p>From other language detection frameworks implemented we know that we should expect the following results.</p>

<ul>
<li><p>The top 300 or so N-grams are almost always highly correlated to the language. Thus the language model from a sporty document will be very similar to the language model generated from a political document. This gives us confidence that if we train the system on the Declaration of Human Right we will still be able to classify documents to the correct language even though they might have completely different topics.</p></li>
<li><p>The highest ranking N-grams are mostly uni-grams and simply reflect the distribution of characters in a language. After uni-grams N-grams representing prefixes and suffixes should be the most popular.</p></li>
<li><p>Starting at around rank 300 or so an N-gram frequency profile begins to become specific to the topic. We will need to optimise this by training the system.</p></li>
</ul><h1>How to compare N-gram models</h1>

<p>Given that we have create the language profile and also performed the pre-processing steps discussed in Section 4 how do we actually compare two profiles. This step is simple, what we do is we take the document profile and calculate a simple rank-order statistic which we call the “out of place” measure. This measure determines how far out of place an N-gram in one profile is from its place in another profile. For example given the following:</p>

<ul>
<li>English Language Profile [TH, ING, ON, ER, AND, ED] </li>
<li>Sample Document Profile [TH, ER, ON, LE, ING, AND]</li>
</ul><p>The out-of-place rank would be 0 + 2 + 0 + K + 3 + 17. K represents a fixed cost for an N-gram which is not found.
In order to classify a sample document we compute the overall distance measure between the document profile and the language profile for each language using the out-of-place measure and then pick the language which has the smallest difference. Alternatively we could also rank them and give the ranked results to the user.</p>

<h1>Setup</h1>

<ul>
<li>Bash &gt;= 4 (Current)</li>
<li>Awk (Current)</li>
<li>Python (Future)</li>
<li>SQLAlchemy (Future)</li>
</ul><h1>Run</h1>

<p>In order to run the language detection module go to the scripts folder and run </p>

<pre><code>./language_detect.sh &lt;text&gt; &lt;ngram size&gt; &lt;number of ngrams to consider&gt;  | sort -k 2 -n
</code></pre>

<p>This will rank a subset of 23 languages with the most likely language first.   This is the result </p>

<p>E.g. </p>

<pre><code>./language_detect.sh "I really think this should work" 3 1000 | sort -k 2 -n
</code></pre>

<p>Should output something like</p>

<pre><code>eng 1999
lux 5049
ger 5396
dut 5836
dns 6106
hng 6134
lit 6697
frn 6727
ukr 6879
ltn1 7052
yps 7160
mls 7061
rum 7388
por 7390
ltn 7405
itn 7406
rmn1 7454
spn 7463
czc 8135
lat 8241
jpn 9200
rus 9200
grk 9200
</code></pre>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/cloudmark">cloudmark</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-32781094-2");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

  </body>
</html>