{"tagline":"Language Detection using an N-Gram rank order algorithm.  ","body":"# Language Detection\r\n\r\n> Text Adapted from [here](http://www.lrec-conf.org/proceedings/lrec2010/pdf/279_Paper.pdf)\r\n\r\nThis project will outline an N-Gram based language detection using the rank order method described [here](http://www.lrec-conf.org/proceedings/lrec2010/pdf/279_Paper.pdf), It is meant to give the necessary context to understand what is happening and well as a valid implementation. The system is based on calculating and comparing language profiles of N-gram frequencies. First we use the system to compute N-gram profiles on training data files written in target languages - language profiles. Given a novel document we want to classify, we compute the N-gram profile of this document document profile and compute the distance between the document and language profiles.\r\n\r\n# So what exactly is an N-Gram? \r\n\r\nN-Grams is an N-character slice of a longer string. Typically one would slice the string into a set of overlapping N-grams. In our system we will use N-Grams of various lengths simultaneously. We will also append blanks to the beginning and end of strings in order to help with matching the beginning-of-word and end-of-word situations. We will represent this using the _ character. Given the word \"TEXT\" we would obtain the following N-Grams\r\n  \r\n* bi-grams _T, TE, EX, XT, T_\r\n* tri-grams _TE, TEX, EXT, XT_, T__ \r\n* quad-grams _TEX, TEXT, EXT_, XT__, T___\r\n  \r\nIn general a string of length k, padded with blanks, will have k + 1 bi-grams, k + 1 tri-grams, k + 1 quad-grams and so on.\r\n\r\n# How does this all work?\r\n\r\nHuman language has some words which occur more than others. For example you can imagine that in a document the word the will occur more frequently than the word aardvark. More over there is always a small set of words which dominates most of the language in terms of frequency of use. This is true both for words in a particular language and also for words in a particular category. Thus words which appear in a sporty document will be different from the words that appear in a political document and words which are in the English language will obviously be different from words which are in the French language.\r\nWell it turns out that these small fragments of words (N-grams) also obey (approximately) the same property. Thus given a document in a particular language we will always find a set of N-grams which dominate and these set of N-grams will be different for each language. Since we are using small fragments of text we are not very susceptible to noise making our detection more resilient.\r\n\r\n# Pre-processing\r\n\r\nGeneration of the language profiles is easy. Given an input document the following steps need to be performed.\r\n\r\n* Split the input document into separate tokens consisting only of letters and apostrophes. Digits and punctuation should be discarded.\r\n\r\n* Remove any extra spacing and make sure that there is always a letter before a punctuation mark. Thus a. is good while a . is bad\r\n\r\n* Spit the document into three parts; train, validation and test in the ratio 0.7, 0.2, and 0.1 respectively. Put the partitioned document into three separate folders; train, validate and test. \r\n  \r\n* Scan down each token generating all possible N-grams, for N = 1 to 5. Use positions that span the padding blanks as well.  \r\n\r\n* Use a hash table to find the counter for the particular N-gram and incrementing it. When done sort the counts in reverse order by the number of occurrences and take the top n N-Grams (limit). The result is the N-gram frequency profile of the language.\r\n\r\n# What to expect\r\n\r\nFrom other language detection frameworks implemented we know that we should expect the following results.\r\n\r\n* The top 300 or so N-grams are almost always highly correlated to the language. Thus the language model from a sporty document will be very similar to the language model generated from a political document. This gives us confidence that if we train the system on the Declaration of Human Right we will still be able to classify documents to the correct language even though they might have completely different topics.\r\n\r\n* The highest ranking N-grams are mostly uni-grams and simply reflect the distribution of characters in a language. After uni-grams N-grams representing prefixes and suffixes should be the most popular.\r\n\r\n* Starting at around rank 300 or so an N-gram frequency profile begins to become specific to the topic. We will need to optimise this by training the system.\r\n\r\n# How to compare N-gram models\r\n\r\nGiven that we have create the language profile and also performed the pre-processing steps discussed in Section 4 how do we actually compare two profiles. This step is simple, what we do is we take the document profile and calculate a simple rank-order statistic which we call the “out of place” measure. This measure determines how far out of place an N-gram in one profile is from its place in another profile. For example given the following:\r\n\r\n* English Language Profile [TH, ING, ON, ER, AND, ED] \r\n* Sample Document Profile [TH, ER, ON, LE, ING, AND]\r\n\r\nThe out-of-place rank would be 0 + 2 + 0 + K + 3 + 17. K represents a fixed cost for an N-gram which is not found.\r\nIn order to classify a sample document we compute the overall distance measure between the document profile and the language profile for each language using the out-of-place measure and then pick the language which has the smallest difference. Alternatively we could also rank them and give the ranked results to the user.\r\n\r\n# Setup \r\n * Bash >= 4 (Current)\r\n * Awk (Current)\r\n * Python (Future)\r\n * SQLAlchemy (Future)\r\n\r\n\r\n# Run\r\nIn order to run the language detection module go to the scripts folder and run \r\n\r\n    ./language_detect.sh <text> <ngram size> <number of ngrams to consider>  | sort -k 2 -n\r\n\r\nThis will rank a subset of 23 languages with the most likely language first.   This is the result \r\n\r\nE.g. \r\n\r\n    ./language_detect.sh \"I really think this should work\" 3 1000 | sort -k 2 -n\r\n \r\nShould output something like\r\n\r\n    eng 1999\r\n    lux 5049\r\n    ger 5396\r\n    dut 5836\r\n    dns 6106\r\n    hng 6134\r\n    lit 6697\r\n    frn 6727\r\n    ukr 6879\r\n    ltn1 7052\r\n    yps 7160\r\n    mls 7061\r\n    rum 7388\r\n    por 7390\r\n    ltn 7405\r\n    itn 7406\r\n    rmn1 7454\r\n    spn 7463\r\n    czc 8135\r\n    lat 8241\r\n    jpn 9200\r\n    rus 9200\r\n    grk 9200\r\n    \r\n    ","google":"UA-32781094-2","note":"Don't delete this file! It's used internally to help with page regeneration.","name":"Language-detect"}